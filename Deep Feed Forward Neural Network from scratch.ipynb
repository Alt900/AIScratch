{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f71dbfc-765e-40d1-961f-f7d4e9de253d",
   "metadata": {},
   "source": [
    "# A deep feed forward neural network from scratch using numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c758ce-35ef-41a0-8b9e-12d5a67350b7",
   "metadata": {},
   "source": [
    "The more I sifted through resources for the duration of this process I found time after time little to no resources that does an absolute full cover of neural networks from scratch, the driving mathematics behind it, and an abstracted perspective on the mechanisms involved. This notebook aims to provide exactly that and leave little to no questions about the complete basics of neural networks and their upscaled cousins, deep neural networks. For a more surface level introduction I already published another [jupyter notebook covering what concepts to understand before reading this notebook](https://github.com/Alt900/AIScratch/blob/main/Introduction%20to%20Artificial%20Intelligence.ipynb) introducing the concepts discussed in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b06d0-6a10-4396-aeda-bd55dc3abd53",
   "metadata": {},
   "source": [
    "## The scope of this project\n",
    "There will be an intense amount of programming going on in this notebook so in order to make this notebook more readable and digestible here is how the notebook will be proceeding:\n",
    "\n",
    "    - Data loading, normalization, and separating \n",
    "\t- Constructing the init dunder method and initializing hyperparameters, activation functions and initialization methods\n",
    "\t- initializing an arbitrary number of layers for our neural network\n",
    "\t- setup the training function calculating z states and applying activation functions\n",
    "\t- Establish a backpropagation routine and parameter updates \n",
    "    - Build a prediction function that performs one forward pass with the updated weights and returns the highest probability. \n",
    "Throughout each section there will be mathematical explanations and example code that might be able to help understand the processes going on behind this network, it is not necessary to read however I highly recommend it as it provides another angle of understanding to how these networks function and what their limitations are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889622d8-50c3-4aab-95ff-c03601315644",
   "metadata": {},
   "source": [
    "### The first steps\n",
    "This process is performed at the end of the file but will be performed first when the class is imported outside of the main file. Before throwing data towards the network it is a good practice to normalize it and split it into batches, for this network we will only be working with one batch at a time however when a multithreading or GPU acceleration system is implemented later down the line batches will be introduced. For now we are taking a typical batch size of 32 images and feeding it into the network for training and weight adjustments. In the code below we are using the Z-score normalization method notated by $Z=\\frac{x-μ}{σ}$ where $μ=\\frac{\\sum_{i=0}^{n}}{n}$ and $σ=\\sqrt{\\frac{\\sum_{i=0}^{n}(x_i-μ)^2}{n}}$ and. $n$ being the size of the image and $x_i$ being the grayscale value on index $i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f7de3e-5175-426b-81d4-3f2745cddd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.array(pd.read_csv(\"train.csv\"))\n",
    "np.random.shuffle(data)\n",
    "m,n=data.shape\n",
    "data.T \n",
    "Y_train=np.array([int(data[x][:1]) for x in range(round(len(data)*.00076))])\n",
    "X_train=np.array([data[x][1:]-np.mean([data[x][1:])/np.std([data[x][1:]) for x in range(round(len(data)*.00076))])\n",
    "Y_test=np.array([int(data[x][:1]) for x in range(round(len(data)*.00078))])[-1]\n",
    "X_test=np.array([data[x][1:]-np.mean([data[x][1:])/np.std([data[x][1:]) for x in range(round(len(data)*.00078))])[-1]#1 image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9476aa-f024-4580-9777-46746038a483",
   "metadata": {},
   "source": [
    "In the code above we load the MNIST dataset from a CSV file [available as train.csv in the data sub-folder of this repo](https://github.com/Alt900/AIScratch/tree/main/Data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67f98339-8df2-499b-9d93-d4b15b730f9d",
   "metadata": {},
   "source": [
    "### Imports and pre-initialization\n",
    "Before performing any sort of forward pass operations we need to set up the neural network class with its set of hyperparameters, initialization techniques, activation functions, and network state caches. Since we are using this as a base for the next network, a convolutional neural network, the label vector is processed into a one-hot encoded vector for multi-classification problems such as image classification or object detection. From there we initialize the hyperparameters into the network's variables, define the Softmax, Tanh, ReLU, and Sigmoid activation functions with their corresponding derivative functions, initialization techniques He and Xavier/Xavier Glorot, and techniques to store activated states, z_states, weights, bias, neuron counts, and activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15beb334-9253-44de-bad8-d35b646bea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Network():\n",
    "    def __init__(self,\n",
    "        input_vector,\n",
    "        True_Label,\n",
    "        label_vector,\n",
    "        alpha,#learning rate for backpropagation\n",
    "        epochs,\n",
    "        seed\n",
    "    ):\n",
    "        #input processing\n",
    "        self.input_vector=input_vector\n",
    "        #one-hot encode the true value labels and the labels\n",
    "        encoded=np.zeros((len(label_vector),int(max(label_vector))+1))\n",
    "        encoded[np.arange(len(label_vector)),label_vector]=1\n",
    "        self.label_vector=encoded\n",
    "        encoded=np.zeros((len(True_Label),int(max(True_Label))+1))\n",
    "        encoded[np.arange(len(True_Label)),True_Label]=1\n",
    "        self.True_Label=encoded\n",
    "\n",
    "        #hyperparamaters\n",
    "        self.alpha=alpha\n",
    "        self.epochs=epochs\n",
    "\n",
    "        #set a seed for numpy generation to get reproducable results from random weight and bias generation\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.initialization={\n",
    "            \"Xavier\":lambda i,o: np.random.uniform(low=-np.sqrt(6/(i+o)),high=np.sqrt(6/(i+o)),size=(o,i)),\n",
    "            \"He\":lambda i,o: np.random.uniform(low=-np.sqrt(6/i),high=np.sqrt(6/i),size=(o,i))\n",
    "        }\n",
    "\n",
    "        self.Activations={\n",
    "            \"Tanh\": lambda x,derivative :\n",
    "                1-np.tanh(x)**2 if derivative \\\n",
    "                    else np.tanh(x)\n",
    "            ,\n",
    "\n",
    "            \"ReLU\": lambda zstate,derivative : np.array(\n",
    "                [0 if x<0 else 1 for x in zstate] if derivative \\\n",
    "                    else [0 if x<0 else x for x in zstate]\n",
    "            ),\n",
    "\n",
    "            \"Sigmoid\": lambda x, derivative : \n",
    "                1/1+np.exp(x)*(1-(1/1+np.exp(x))) if derivative \\\n",
    "                else 1/1+np.exp(-x)\n",
    "        }\n",
    "\n",
    "        self.preinitdict={\n",
    "            \"function\":None,#pointer to a lambda function chosen from the activation functions map\n",
    "            \"neurons\":0,#int\n",
    "            \"weights\":None,#2D matrix\n",
    "            \"bias\":None,#1D vector\n",
    "            \"z_state\":None,#1D array\n",
    "            \"activated_state\":None#1D array\n",
    "        }\n",
    "\n",
    "        self.network={\n",
    "            \"input_layer\":self.input_vector,\n",
    "            \"output_layer\":dict(self.preinitdict),\n",
    "            \"hidden_layers\":{},\n",
    "            \"LossFunction\":None\n",
    "        }\n",
    "        self.network[\"output_layer\"][\"neurons\"]=len(label_vector)\n",
    "\n",
    "    def Softmax(self,vector,derivative=False):\n",
    "        if derivative:\n",
    "            n=len(vector)\n",
    "            SM=self.Softmax(vector,derivative=False)\n",
    "            JM=np.empty((n,n),dtype=type(SM))\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    if i==j:\n",
    "                        JM[i,j]=SM[i]*(1-SM[j])\n",
    "                    else:\n",
    "                        JM[i,j]=-SM[i]*SM[j]\n",
    "            return JM\n",
    "        else:\n",
    "            stable_exponential=np.exp(vector-np.max(vector))#prevent infinite overflow and underflow\n",
    "            return stable_exponential/np.sum(stable_exponential)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6842942c-7fdd-45f4-92b4-01674b1ac401",
   "metadata": {},
   "source": [
    "### Weight/Bias initialization techniques and the activation functions\n",
    "Weight/Bias initialization techniques are used to ensure a more stable training rather than using random weights or zero weights. Both Xavier/Xavier Glorot and He initialization techniques are based on normal distributions (although Xavier can be based on normal or uniform distribution) however their differences lie in what activation functions are used for each layer in the network. Xavier accepts all activation functions but He initialization is specifically built for the ReLU/Leaky ReLU function in order to prevent the vanishing gradient problem by scaling the weights to a factor of 2/n (n being the number of neurons in the layer). This prevents the dying ReLU neuron problem where neurons get stuck in a state of always outputting zero due to negative weight initialization that could potentially arise from Xavier initialization. The network initialization function created later on already automatically determines what initialization method to use on a layer-to-layer basis so you do not have to worry about adding ReLU anywhere in the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012d958c-9d53-422d-9e06-c0b650b4bc81",
   "metadata": {},
   "source": [
    "### The math of weight/bias initialization\n",
    "As stated previously under Xavier/Xavier Glort initialization there can be two types of distributions used, the uniform or normal distribution ranging from [-x,x]. For uniform distribution the formula is $x=\\sqrt{\\frac{6}{i+o}}$ Where $i$ and $o$ are the lengths of the input and output neurons or the corresponding connecting weights from the previous layer to the current layer. And for normal distribution $x=\\sqrt{\\frac{2}{i+o}}$. In the examples provided above I used uniform distribution rather than normal but if you want to experiment simply take the line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7dfeb8-4a1e-4da1-8ca6-bb581e5862a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Xavier\":lambda i,o: np.random.uniform(low=-np.sqrt(6/(i+o)),high=np.sqrt(6/(i+o)),size=(o,i)),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefea482-072c-4e60-93d8-a19161fe3d4e",
   "metadata": {},
   "source": [
    "And change it to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4879ac12-ed33-4405-afc9-fd6baf6b262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Xavier\":lambda i,o: np.random.normal(low=-np.sqrt(2/(i+o)),high=np.sqrt(2/(i+o)),size=(o,i)),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2a3e73-f031-4d9a-8c70-f31e700d7444",
   "metadata": {},
   "source": [
    "### Initializing each layer \n",
    "Since this is a deep neural network with an arbitrary number of layers, neurons, input size, and output label size we need to initialize each layer dynamically and iteratively for each layer specified in the layercount variable passed in the layer initialization function. Another parameter we pass would be the neuron distribution list or simply a number of neurons. If a list of equal size to the number of layers is passed then the function would simply accept the list, otherwise a list of neuron distributions is generated starting in reversed order with the number of neurons specified, then performing floor division (to ensure the resulting operating is a int) with the denominator being the layer number. I'E: 200 neurons over 2 layers = H1(100), H2(200). The weight initializations are also based on these lists as well. Similarly a list of activation functions can be passed instead of a single string or universal activation function, allowing for more modularity and low-level control/tuning of the network, So for a 3 layer network you could setup Tanh in H1, Sigmoid in H2, and ReLU in H3.\n",
    "\n",
    "\n",
    "As previously mentioned when mapping out lambda functions in a dict for weight/bias initialization techniques, the layer initialization will also contain conditionals checking exactly what type of function is being used in the neural network. This ensures an extra layer of stability in the network since if we are using ReLU or LeakyReLU we want to initialize the weights/bias based on the He initialization function rather than Xavier initialization and risk negative weights causing the dying ReLU neuron problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67aebce-8e8a-4716-a273-ef804ca1533c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_layers(self,neurondist,layercount,activation):\n",
    "    self.layercount=layercount+1\n",
    "    if type(neurondist)==int:\n",
    "        neurondist=[neurondist//y for y in reversed([x for x in range(1,self.layercount)])]\n",
    "    if type(activation)==str:\n",
    "        if activation==\"Softmax\":\n",
    "            print(\"Cannot add softmax activation for hidden layer activation functions.\")\n",
    "            exit()\n",
    "        if type(activation)==list and len(activation)!=self.layercount:\n",
    "            print(\"The list of activation functions are not complete\")\n",
    "            exit()\n",
    "        else:\n",
    "            activation=[activation for y in range(1,self.layercount)]\n",
    "            \n",
    "    for x in range(1,self.layercount):\n",
    "        self.network[\"hidden_layers\"][x]=dict(self.preinitdict)\n",
    "        self.network[\"hidden_layers\"][x][\"neurons\"]=neurondist[x-1]\n",
    "        args=[[neurondist[x-1],len(self.input_vector[0])] if x==1 else [neurondist[x-1],neurondist[x-2]]]\n",
    "        self.network[\"hidden_layers\"][x][\"function\"]=self.Activations[activation[x-1]]\n",
    "        if activation[x-1]==\"ReLU\":\n",
    "            self.network[\"hidden_layers\"][x][\"weights\"]=self.initialization[\"He\"](args[0][1],args[0][0])\n",
    "            self.network[\"hidden_layers\"][x][\"bias\"]=self.initialization[\"He\"](1,args[0][0])\n",
    "        else:\n",
    "            self.network[\"hidden_layers\"][x][\"weights\"]=self.initialization[\"Xavier\"](args[0][1],args[0][0])\n",
    "            self.network[\"hidden_layers\"][x][\"bias\"]=self.initialization[\"Xavier\"](1,args[0][0])\n",
    "    self.network[\"output_layer\"][\"weights\"]=self.initialization[\"Xavier\"](args[0][0],len(self.label_vector))\n",
    "    self.network[\"output_layer\"][\"bias\"]=self.initialization[\"Xavier\"](1,len(self.label_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a628c81-ea21-432f-8993-707dbe5e623d",
   "metadata": {},
   "source": [
    "### Z state calculations, applications of activation functions, and the forward pass process\n",
    "Now it's finally time to get into the driving forces for sequential models from ChatGPT, Google Gemini, and Microsoft Copilot to Google translate, Siri, and Apple Vision Pro with augmented reality. Starting at the first computation performed, the z state, consisting of the dot product of the previous layers output and the weights with a bias term added, mathematically the equation is as follows:\n",
    "$$\\sum_{i=0}^{OutputLength} (O_1*W_1+O_2*W_2 ... O_N*W_N)+Bias$$ Since the activation function is completely up to you the exact calculations will vary with each activation function, however since I only included three activation functions for now here is how each calculation should look with their corresponding derivative which we will use later on in the network.\n",
    "\n",
    "Sigmoid:\n",
    "$$σ = \\frac{1}{1+e^{-x}}$$ $$σ'=σ*(1-σ)$$ Tanh: $$Tanh=\\frac{{e^x}-{e^{-x}}}{{e^{-x}}+{e^x}}$$ $$Tanh'=1-(\\frac{{e^x}-{e^{-x}}}{{e^{-x}}+{e^x}})^2$$\n",
    "ReLU: $$ReLU=max(0,x)$$ $$ReLU'=\\begin{cases}0&if x < 0 \\\\1&if x > 0\\end{cases}$$ There are also a few other activation functions with different use cases you could attempt to implement if messing around with the code is more your style, for example the swish activation function (also known as the scaled exponential linear unit with a shift, you will see why below) developed by Google in 2017: $$Swish = (x*σ) *(β*x)$$ $β$ in this instance is a hyperparameter itself acting as a function shape control factor, effectively dictating how the function will [squash function outputs](https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/model_agnostic/Squashing%20Effect.html) allowing for a greater control over what data features are abstracted in the network.\n",
    "\n",
    "\n",
    "From here onwards the process is pretty automatic, each z state calculation is applied to each neuron in a layer, its output taken and fed into an activation function for each neuron, producing an output the size of the layers neurons. Then each output of that layer is fed into the z state calculation of the next layer as x, and so on so forth. This process is always automated on larger systems and that automated system is what I provided below, however if you're struggling to understand exactly how this process is structured I would highly recommend messing with the fundamentals on a smaller scale.\n",
    "\n",
    "Credit to [WarshipConn](https://github.com/WarshipConn?tab=overview&from=2024-03-01&to=2024-03-31) for conjuring up some smaller scale code that operates with single and multiple neurons on a single layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78421ea8-ed78-4eb8-bbcd-17dddf87cd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for a singl neuron\n",
    "x=np.array((1,2,3))#the output of the previous layer or input layer data\n",
    "w=np.array((4,5,6))#a single weight values list \n",
    "b=7#the bias term\n",
    "\n",
    "def f(x):\n",
    "    return 1/(1=np.exp(-x))#sigmoid activation function\n",
    "\n",
    "def neuron(x, w, b, f):\n",
    "    if len(x) == len(w):\n",
    "        return f(sum(x*w) + b)\n",
    "    else:\n",
    "        return \"Missing weights or bias\"\n",
    "\n",
    "###for multiple neurons###\n",
    "x = np.array((1, 2))\n",
    "w = np.array(((1,2), (3,4), (5,6))) #3 neurons, 2 input & weight pairs each\n",
    "b = np.array((1, 2, 3))\n",
    "\n",
    "def neurons(x, w, b, f):\n",
    "    if w.shape[0] == len(b):\n",
    "        if w.shape[1] == len(x):\n",
    "\n",
    "            xw = np.array([sum(i) for i in x*w])\n",
    "            result = [0] * w.shape[0]\n",
    "            \n",
    "            for i in range(w.shape[0]):\n",
    "                result[i] = f(xw[i] + b[i])\n",
    "\n",
    "            return result\n",
    "        else:\n",
    "            return \"Mismatch between inputs and weights\"\n",
    "    else:\n",
    "        return \"Missing weights or bias for some neurons\"\n",
    "print(neurons(x, w, b, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbfe627-fd46-4287-a63e-3e6b705ea225",
   "metadata": {},
   "source": [
    "### The automated code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb3bfb8-4923-4790-8071-ce1e80349a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(self):\n",
    "    for _ in range(0,self.epochs):\n",
    "        for i in range(len(self.input_vector)):\n",
    "            for x in range(1,self.layercount):\n",
    "                z=np.empty(shape=self.network[\"hidden_layers\"][x][\"neurons\"])\n",
    "                for y in range(self.network[\"hidden_layers\"][x][\"neurons\"]):\n",
    "                    if x==1:\n",
    "                        z.flat[y]=np.sum(np.dot(\n",
    "                            self.network[\"hidden_layers\"][x][\"weights\"][y],\n",
    "                            self.input_vector[i]\n",
    "                        )+self.network[\"hidden_layers\"][x][\"bias\"][y])\n",
    "                    else:\n",
    "                        z.flat[y]=np.sum(np.dot(\n",
    "                            self.network[\"hidden_layers\"][x][\"weights\"][y],\n",
    "                            self.network[\"hidden_layers\"][x-1][\"activated_state\"]\n",
    "                        )+self.network[\"hidden_layers\"][x][\"bias\"][y])\n",
    "                self.network[\"hidden_layers\"][x][\"z_state\"]=z\n",
    "                self.network[\"hidden_layers\"][x][\"activated_state\"]=self.network[\"hidden_layers\"][x][\"function\"](z,False)\n",
    "\n",
    "            z=np.empty(shape=self.network[\"output_layer\"][\"neurons\"])\n",
    "            for y in range(self.network[\"output_layer\"][\"neurons\"]):\n",
    "                z.flat[y]=np.sum(np.multiply(\n",
    "                    self.network[\"output_layer\"][\"weights\"][y],\n",
    "                    self.network[\"hidden_layers\"][x][\"activated_state\"]\n",
    "                )+self.network[\"output_layer\"][\"bias\"][y])\n",
    "            self.network[\"output_layer\"][\"z_state\"]=z\n",
    "            self.network[\"output_layer\"][\"activated_state\"]=self.Softmax(z)\n",
    "            self.backpropagation(i)\n",
    "\n",
    "        Loss=-np.sum(self.True_Label[i] * np.log(self.network[\"output_layer\"][\"activated_state\"]))\n",
    "        plt.plot(_,Loss,'go')\n",
    "        print(f'Loss: {Loss} on epoch {_}')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Gradient descent direction\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf6ae23-93a5-4313-9fd5-acc87bb2ffba",
   "metadata": {},
   "source": [
    "### Calculating loss\n",
    "In order to understand how accurate or inaccurate our network is we need a loss function to generate that metric. The choice of loss is categorical cross-entropy loss, again since this is a multi-classification network we will be dealing with softmax, and CCE is one of the few options we have aside from its variants sparse CCE and logit CCE. CCE's equation is as follows and only consists of three operations we need to perform:\n",
    "$$CCE=-\\sum_{i}^{C}=t_i*log(f(Z)_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ea27c4-6514-4f98-a71a-4f49c548b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss=-np.sum(self.True_Label[i] * np.log(self.network[\"output_layer\"][\"activated_state\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eea80d0-0ede-4485-838d-41e1ab14bd31",
   "metadata": {},
   "source": [
    "There are plenty of other loss functions to look at such as mean squared error/MSE, Hinge loss, Huber loss, smooth mean absolute error/SMAE, ect but CCE and its variants are typically the only loss functions used in multi-classification problems as it deals with probabilities structured around the softmax activation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72bc341-89e1-4451-8521-94685ee3420e",
   "metadata": {},
   "source": [
    "### Backpropagation, the steepest learning curve in AI for beginners\n",
    "Compared to how the first half of this network has been constructed it seems pretty straight forward, just compute the z state and apply the activation function, backpropagation should be the same but in reverse right? In essence it is but there are a lot of mathematical complexities behind the algorithm and for good reason, backpropagation is the mechanism behind how artificial intelligence actually can learn. It involves gradients, how gradient descent works and the relevant multivariable calculus, derivatives, the chain rule of calculus, linear algebra, and a mix of matrix and vector operations. If you already have some idea of how calculus works such as the chain rule and how to find derivatives of functions might see why this starts to become a problem. In the forward pass operations we calculate the Z state first, taking inputs X, W, and B then producing a single output, that output is then fed into an activation function, which then serves as the input for the next Z state and so on. This introduces a (technically 2) new element(s) for beginners in Calculus, multivariable composite functions. In the case of neural networks it's multivariable composite function looks like $f(z(({\\sum_{i=0}^i} X_i*W_i)+B_i))$ meaning our derivative will start like $f'(z'(({\\sum_{i=0}^i} X_i*W_i)+B_i))$ This complicates the chain rule but can be generally addressed with the chain rule as $\\frac{∂E^L}{∂A^L}*\\frac{∂A^L}{∂Z^L}*\\frac{∂Z^L}{∂W^L}$. That only solves one problem however, this only applies to one layer of the network or the output layer, in order to perform backpropagation properly we have to send this resulting change in the error function with respect to the weights back through the rest of the layers. If we had say a network with 10 hidden layers and an output layer and we wanted to find the effect the weights in layer 8 has on the loss function and subsequently the proceeding layers. The chain rule would then be extended further to this:\n",
    "$$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂A^{10}}*\\frac{∂A^{10}}{∂Z^{10}}*\\frac{∂Z^{10}}{∂A^9}*\\frac{∂A^9}{∂Z^9}*\\frac{∂Z^9}{∂A^8}*\\frac{∂A^8}{∂Z^8}*\\frac{∂Z^8}{∂W^8}$$ and if you are curious to see from the output layer to the first hidden layer: $$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂A^{10}}*\\frac{∂A^{10}}{∂Z^{10}}*\\frac{∂Z^{10}}{∂A^9}*\\frac{∂A^9}{∂Z^9}*\\frac{∂Z^9}{∂A^8}*\\frac{∂A^8}{∂Z^8}*\\frac{∂Z^8}{∂A^7}*\\frac{∂A^7}{∂Z^7}*\\frac{∂Z^7}{∂A^6}*\\frac{∂A^6}{∂Z^6}*\\frac{∂Z^6}{∂A^5}*\\frac{∂A^5}{∂Z^5}*\\frac{∂Z^5}{∂A^4}*\\frac{∂A^4}{∂Z^4}*\\frac{∂Z^4}{∂A^3}*\\frac{∂A^3}{∂Z^3}*\\frac{∂Z^3}{∂A^2}*\\frac{∂A^2}{∂Z^2}*\\frac{∂Z^2}{∂A^1}*\\frac{∂A^1}{∂Z^1}*\\frac{∂Z^1}{∂W^1}$$ This [multivariable multicomposite function chain rule](https://math.libretexts.org/Bookshelves/Calculus/Calculus_(OpenStax)/14%3A_Differentiation_of_Functions_of_Several_Variables/14.05%3A_The_Chain_Rule_for_Multivariable_Functions) is a mind melter to get around sometimes especially when beginning with AI and/or ML. But getting into the depths of the equation and what each component is it takes a huge load off someones mind trying to just take the equations head on, the relevant calculus is a handfull to explain and would make this notebook extremely long so I highly reccomend checking out [3B1B's series on calculus](https://www.youtube.com/watch?v=WUvTyaaNkzM&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr), he does an amazing job of making complex topics like calculus easy to digest.\n",
    "\n",
    "\n",
    "Now how do we even begin trying to implement this into a neural network? Well as previously mentioned once it's boiled down it's easier to understand, we want to find how much the Loss function changes with respect to a targeted parameter, bias or weights, and update our weights based on the change that causes the greatest decrease in the loss function's output. This is the idea we need to implement for each weight, but we don't have to re-calculate partial derivatives for each layer we go through, a more computationally friendly approach would be to calculate the gradients for the current layer and store it to be used as a input to the next stage of computation in the chain rule for the next layer. We can achieve this by instantizing the first weight gradient in the output layer, taking the result of $\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂W^{11}}$ and multiplying the next partial derivative $\\frac{∂A^{10}}{∂Z^{10}}$ note that : $\\frac{∂Z^{11}}{∂W^{11}}\\rightarrow\\frac{∂Z^{11}}{∂A^{10}}$. Proceeding with similar calculation steps used in the output layer we can replace the instantized gradient of $\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂W^{11}}$ with $\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂A^{10}}*\\frac{∂A^{10}}{∂Z^{10}}*\\frac{∂Z^{10}}{∂W^10}$. Repeating this process for each layer and applying the relevant gradient calculations to the parameters of that layer we effectively performed the entire chain rule by the time we finish the backpropagation process. This process mathematically for each layer and each parameter would look like the following without caching the gradient:\n",
    "\n",
    "Again noting: $\\frac{∂Z^L}{∂W^L} \\rightarrow \\frac{∂Z^L}{∂A^{L-1}}$\n",
    "#### Without Cache\n",
    "$DW^{11}$=\n",
    "$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂W^{11}}$\n",
    "\n",
    "$DB^{11}$=\n",
    "$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂B^{11}}$\n",
    "\n",
    "$DW^{10}$=\n",
    "$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂A^{10}}*\\frac{∂A^{10}}{∂Z^{10}}*\\frac{∂Z^{10}}{∂W^{10}}$\n",
    "\n",
    "$DB^{10}$=\n",
    "$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂A^{10}}*\\frac{∂A^{10}}{∂Z^{10}}*\\frac{∂Z^{10}}{∂B^{10}}$\n",
    "\n",
    "$DW^9$=\n",
    "$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂A^{10}}*\\frac{∂A^{10}}{∂Z^{10}}*\\frac{∂Z^{10}}{∂A^9}*\\frac{∂A^9}{∂Z^9}*\\frac{∂Z^9}{∂W^9}$\n",
    "\n",
    "$DB^9$=\n",
    "$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂A^{10}}*\\frac{∂A^{10}}{∂Z^{10}}*\\frac{∂Z^{10}}{∂A^9}*\\frac{∂A^9}{∂Z^9}*\\frac{∂Z^9}{∂B^9}$\n",
    "\n",
    "$DW^8$=\n",
    "$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂A^{10}}*\\frac{∂A^{10}}{∂Z^{10}}*\\frac{∂Z^{10}}{∂A^9}*\\frac{∂A^9}{∂Z^9}*\\frac{∂Z^9}{∂A^8}*\\frac{∂A^8}{∂Z^8}*\\frac{∂Z^8}{∂W^8}$\n",
    "\n",
    "$DB^8$=\n",
    "$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂A^{10}}*\\frac{∂A^{10}}{∂Z^{10}}*\\frac{∂Z^{10}}{∂A^9}*\\frac{∂A^9}{∂Z^9}*\\frac{∂Z^9}{∂A^8}*\\frac{∂A^8}{∂Z^8}*\\frac{∂Z^8}{∂B^8}$\n",
    "\n",
    "$DW^7$=\n",
    "$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂A^{10}}*\\frac{∂A^{10}}{∂Z^{10}}*\\frac{∂Z^{10}}{∂A^9}*\\frac{∂A^9}{∂Z^9}*\\frac{∂Z^9}{∂A^8}*\\frac{∂A^8}{∂Z^8}*\\frac{∂Z^8}{∂A^7}*\\frac{∂A^7}{∂Z^7}*\\frac{∂Z^7}{∂W^7}$\n",
    "\n",
    "$DB^7$=\n",
    "$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂A^{10}}*\\frac{∂A^{10}}{∂Z^{10}}*\\frac{∂Z^{10}}{∂A^9}*\\frac{∂A^9}{∂Z^9}*\\frac{∂Z^9}{∂A^8}*\\frac{∂A^8}{∂Z^8}*\\frac{∂Z^8}{∂A^7}*\\frac{∂A^7}{∂Z^7}*\\frac{∂Z^7}{∂B^7}$\n",
    "\n",
    "$DW^6$=\n",
    "$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂A^{10}}*\\frac{∂A^{10}}{∂Z^{10}}*\\frac{∂Z^{10}}{∂A^9}*\\frac{∂A^9}{∂Z^9}*\\frac{∂Z^9}{∂A^8}*\\frac{∂A^8}{∂Z^8}*\\frac{∂Z^8}{∂A^7}*\\frac{∂A^7}{∂Z^7}*\\frac{∂Z^7}{∂A^6}*\\frac{∂A^6}{∂Z^6}*\\frac{∂Z^6}{∂W^6}$\n",
    "\n",
    "$DB^6$=\n",
    "$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂A^{10}}*\\frac{∂A^{10}}{∂Z^{10}}*\\frac{∂Z^{10}}{∂A^9}*\\frac{∂A^9}{∂Z^9}*\\frac{∂Z^9}{∂A^8}*\\frac{∂A^8}{∂Z^8}*\\frac{∂Z^8}{∂A^7}*\\frac{∂A^7}{∂Z^7}*\\frac{∂Z^7}{∂A^6}*\\frac{∂A^6}{∂Z^6}*\\frac{∂Z^6}{∂B^6}$\n",
    "\n",
    "$DW^5$=\n",
    "$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂A^{10}}*\\frac{∂A^{10}}{∂Z^{10}}*\\frac{∂Z^{10}}{∂A^9}*\\frac{∂A^9}{∂Z^9}*\\frac{∂Z^9}{∂A^8}*\\frac{∂A^8}{∂Z^8}*\\frac{∂Z^8}{∂A^7}*\\frac{∂A^7}{∂Z^7}*\\frac{∂Z^7}{∂A^6}*\\frac{∂A^6}{∂Z^6}*\\frac{∂Z^6}{∂A^5}*\\frac{∂A^5}{∂Z^5}*\\frac{∂Z^5}{∂W^5}$\n",
    "\n",
    "$DB^5$=\n",
    "$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂A^{10}}*\\frac{∂A^{10}}{∂Z^{10}}*\\frac{∂Z^{10}}{∂A^9}*\\frac{∂A^9}{∂Z^9}*\\frac{∂Z^9}{∂A^8}*\\frac{∂A^8}{∂Z^8}*\\frac{∂Z^8}{∂A^7}*\\frac{∂A^7}{∂Z^7}*\\frac{∂Z^7}{∂A^6}*\\frac{∂A^6}{∂Z^6}*\\frac{∂Z^6}{∂A^5}*\\frac{∂A^5}{∂Z^5}*\\frac{∂Z^5}{∂B^5}$\n",
    "\n",
    "$DW^4$=\n",
    "$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂A^{10}}*\\frac{∂A^{10}}{∂Z^{10}}*\\frac{∂Z^{10}}{∂A^9}*\\frac{∂A^9}{∂Z^9}*\\frac{∂Z^9}{∂A^8}*\\frac{∂A^8}{∂Z^8}*\\frac{∂Z^8}{∂A^7}*\\frac{∂A^7}{∂Z^7}*\\frac{∂Z^7}{∂A^6}*\\frac{∂A^6}{∂Z^6}*\\frac{∂Z^6}{∂A^5}*\\frac{∂A^5}{∂Z^5}*\\frac{∂Z^5}{∂A^4}*\\frac{∂A^4}{∂Z^4}*\\frac{∂Z^4}{∂W^4}$\n",
    "\n",
    "$DB^4$=\n",
    "$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂A^{10}}*\\frac{∂A^{10}}{∂Z^{10}}*\\frac{∂Z^{10}}{∂A^9}*\\frac{∂A^9}{∂Z^9}*\\frac{∂Z^9}{∂A^8}*\\frac{∂A^8}{∂Z^8}*\\frac{∂Z^8}{∂A^7}*\\frac{∂A^7}{∂Z^7}*\\frac{∂Z^7}{∂A^6}*\\frac{∂A^6}{∂Z^6}*\\frac{∂Z^6}{∂A^5}*\\frac{∂A^5}{∂Z^5}*\\frac{∂Z^5}{∂A^4}*\\frac{∂A^4}{∂Z^4}*\\frac{∂Z^4}{∂B^4}$\n",
    "\n",
    "$DW^3$=\n",
    "$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂A^{10}}*\\frac{∂A^{10}}{∂Z^{10}}*\\frac{∂Z^{10}}{∂A^9}*\\frac{∂A^9}{∂Z^9}*\\frac{∂Z^9}{∂A^8}*\\frac{∂A^8}{∂Z^8}*\\frac{∂Z^8}{∂A^7}*\\frac{∂A^7}{∂Z^7}*\\frac{∂Z^7}{∂A^6}*\\frac{∂A^6}{∂Z^6}*\\frac{∂Z^6}{∂A^5}*\\frac{∂A^5}{∂Z^5}*\\frac{∂Z^5}{∂A^4}*\\frac{∂A^4}{∂Z^4}*\\frac{∂Z^4}{∂A^3}*\\frac{∂A^3}{∂Z^3}*\\frac{∂Z^3}{∂W^3}$\n",
    "\n",
    "$DB^3$=\n",
    "$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂A^{10}}*\\frac{∂A^{10}}{∂Z^{10}}*\\frac{∂Z^{10}}{∂A^9}*\\frac{∂A^9}{∂Z^9}*\\frac{∂Z^9}{∂A^8}*\\frac{∂A^8}{∂Z^8}*\\frac{∂Z^8}{∂A^7}*\\frac{∂A^7}{∂Z^7}*\\frac{∂Z^7}{∂A^6}*\\frac{∂A^6}{∂Z^6}*\\frac{∂Z^6}{∂A^5}*\\frac{∂A^5}{∂Z^5}*\\frac{∂Z^5}{∂A^4}*\\frac{∂A^4}{∂Z^4}*\\frac{∂Z^4}{∂A^3}*\\frac{∂A^3}{∂Z^3}*\\frac{∂Z^3}{∂B^3}$\n",
    "\n",
    "$DW^2$=\n",
    "$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂A^{10}}*\\frac{∂A^{10}}{∂Z^{10}}*\\frac{∂Z^{10}}{∂A^9}*\\frac{∂A^9}{∂Z^9}*\\frac{∂Z^9}{∂A^8}*\\frac{∂A^8}{∂Z^8}*\\frac{∂Z^8}{∂A^7}*\\frac{∂A^7}{∂Z^7}*\\frac{∂Z^7}{∂A^6}*\\frac{∂A^6}{∂Z^6}*\\frac{∂Z^6}{∂A^5}*\\frac{∂A^5}{∂Z^5}*\\frac{∂Z^5}{∂A^4}*\\frac{∂A^4}{∂Z^4}*\\frac{∂Z^4}{∂A^3}*\\frac{∂A^3}{∂Z^3}*\\frac{∂Z^3}{∂A^2}*\\frac{∂A^2}{∂Z^2}*\\frac{∂Z^2}{∂W^2}$\n",
    "\n",
    "$DB^2$=\n",
    "$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂A^{10}}*\\frac{∂A^{10}}{∂Z^{10}}*\\frac{∂Z^{10}}{∂A^9}*\\frac{∂A^9}{∂Z^9}*\\frac{∂Z^9}{∂A^8}*\\frac{∂A^8}{∂Z^8}*\\frac{∂Z^8}{∂A^7}*\\frac{∂A^7}{∂Z^7}*\\frac{∂Z^7}{∂A^6}*\\frac{∂A^6}{∂Z^6}*\\frac{∂Z^6}{∂A^5}*\\frac{∂A^5}{∂Z^5}*\\frac{∂Z^5}{∂A^4}*\\frac{∂A^4}{∂Z^4}*\\frac{∂Z^4}{∂A^3}*\\frac{∂A^3}{∂Z^3}*\\frac{∂Z^3}{∂A^2}*\\frac{∂A^2}{∂Z^2}*\\frac{∂Z^2}{∂B^2}$\n",
    "\n",
    "$DW^1$=\n",
    "$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂A^{10}}*\\frac{∂A^{10}}{∂Z^{10}}*\\frac{∂Z^{10}}{∂A^9}*\\frac{∂A^9}{∂Z^9}*\\frac{∂Z^9}{∂A^8}*\\frac{∂A^8}{∂Z^8}*\\frac{∂Z^8}{∂A^7}*\\frac{∂A^7}{∂Z^7}*\\frac{∂Z^7}{∂A^6}*\\frac{∂A^6}{∂Z^6}*\\frac{∂Z^6}{∂A^5}*\\frac{∂A^5}{∂Z^5}*\\frac{∂Z^5}{∂A^4}*\\frac{∂A^4}{∂Z^4}*\\frac{∂Z^4}{∂A^3}*\\frac{∂A^3}{∂Z^3}*\\frac{∂Z^3}{∂A^2}*\\frac{∂A^2}{∂Z^2}*\\frac{∂Z^2}{∂A^1}*\\frac{∂A^1}{∂Z^1}*\\frac{∂Z^1}{∂W^1}$\n",
    "\n",
    "$DB^1$=\n",
    "$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂A^{10}}*\\frac{∂A^{10}}{∂Z^{10}}*\\frac{∂Z^{10}}{∂A^9}*\\frac{∂A^9}{∂Z^9}*\\frac{∂Z^9}{∂A^8}*\\frac{∂A^8}{∂Z^8}*\\frac{∂Z^8}{∂A^7}*\\frac{∂A^7}{∂Z^7}*\\frac{∂Z^7}{∂A^6}*\\frac{∂A^6}{∂Z^6}*\\frac{∂Z^6}{∂A^5}*\\frac{∂A^5}{∂Z^5}*\\frac{∂Z^5}{∂A^4}*\\frac{∂A^4}{∂Z^4}*\\frac{∂Z^4}{∂A^3}*\\frac{∂A^3}{∂Z^3}*\\frac{∂Z^3}{∂A^2}*\\frac{∂A^2}{∂Z^2}*\\frac{∂Z^2}{∂A^1}*\\frac{∂A^1}{∂Z^1}*\\frac{∂Z^1}{∂B^1}$\n",
    "\n",
    "\n",
    "#### With Cache\n",
    "\n",
    "$DW^{11}$=\n",
    "$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂W^{11}}$\n",
    "\n",
    "$DB^{11}$=\n",
    "$\\frac{∂E^{11}}{∂A^{11}}*\\frac{∂A^{11}}{∂Z^{11}}*\\frac{∂Z^{11}}{∂B^{11}}$\n",
    "\n",
    "$DW^{10}$=\n",
    "$DW^{11}*\\frac{∂A^{10}}{∂Z^{10}}*\\frac{∂Z^{10}}{∂W^{10}}$\n",
    "\n",
    "$DB^{10}$=\n",
    "$DB^{11}*\\frac{∂A^{10}}{∂Z^{10}}*\\frac{∂Z^{10}}{∂B^{10}}$\n",
    "\n",
    "$DW^9$=\n",
    "$DW{10}^*\\frac{∂A^9}{∂Z^9}*\\frac{∂Z^9}{∂W^9}$\n",
    "\n",
    "$DB^9$=\n",
    "$DB^{10}*\\frac{∂A^9}{∂Z^9}*\\frac{∂Z^9}{∂B^9}$\n",
    "\n",
    "$DW^8$=\n",
    "$DW^9*\\frac{∂A^8}{∂Z^8}*\\frac{∂Z^8}{∂W^8}$\n",
    "\n",
    "$DB^8$=\n",
    "$DB^9*\\frac{∂A^8}{∂Z^8}*\\frac{∂Z^8}{∂B^8}$\n",
    "\n",
    "$DW^7$=\n",
    "$DW^8*\\frac{∂A^7}{∂Z^7}*\\frac{∂Z^7}{∂W^7}$\n",
    "\n",
    "$DB^7$=\n",
    "$DB^8*\\frac{∂A^7}{∂Z^7}*\\frac{∂Z^7}{∂B^7}$\n",
    "\n",
    "$DW^6$=\n",
    "$DW^7*\\frac{∂A^6}{∂Z^6}*\\frac{∂Z^6}{∂W^6}$\n",
    "\n",
    "$DB^6$=\n",
    "$DB^7*\\frac{∂A^6}{∂Z^6}*\\frac{∂Z^6}{∂B^6}$\n",
    "\n",
    "$DW^5$=\n",
    "$DW^6*\\frac{∂A^5}{∂Z^5}*\\frac{∂Z^5}{∂W^5}$\n",
    "\n",
    "$DB^5$=\n",
    "$DB^6*\\frac{∂A^5}{∂Z^5}*\\frac{∂Z^5}{∂B^5}$\n",
    "\n",
    "$DW^4$=\n",
    "$DW^5*\\frac{∂A^4}{∂Z^4}*\\frac{∂Z^4}{∂W^4}$\n",
    "\n",
    "$DB^4$=\n",
    "$DB^5*\\frac{∂A^4}{∂Z^4}*\\frac{∂Z^4}{∂B^4}$\n",
    "\n",
    "$DW^3$=\n",
    "$DW^4*\\frac{∂A^3}{∂Z^3}*\\frac{∂Z^3}{∂W^3}$\n",
    "\n",
    "$DB^3$=\n",
    "$DB^4*\\frac{∂A^3}{∂Z^3}*\\frac{∂Z^3}{∂B^3}$\n",
    "\n",
    "$DW^2$=\n",
    "$DW^3*\\frac{∂A^2}{∂Z^2}*\\frac{∂Z^2}{∂W^2}$\n",
    "\n",
    "$DB^2$=\n",
    "$DB^3*\\frac{∂A^2}{∂Z^2}*\\frac{∂Z^2}{∂B^2}$\n",
    "\n",
    "$DW^1$=\n",
    "$DW^2*\\frac{∂A^1}{∂Z^1}*\\frac{∂Z^1}{∂W^1}$\n",
    "\n",
    "$DB^1$=\n",
    "$DB^2*\\frac{∂A^1}{∂Z^1}*\\frac{∂Z^1}{∂B^1}$\n",
    "\n",
    "Just comparing the two makes it generally obvious why caching is a very important process, the code below is the cache version. I would provide the full version but that's an extremely restrictive and rigorous process, as well as the code itself would be very large and impractical to implement given arbitrary layer sizes. With that being said here is the vanilla backpropagation algorithm with a cache: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db02d7-8f2a-452d-8a71-e2ff11f8be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(self,i):\n",
    "    DL=self.network[\"output_layer\"][\"activated_state\"]-self.True_Label[i]\n",
    "    for x in reversed(range(1,len(self.network[\"hidden_layers\"])+1)):\n",
    "        if x==len(self.network[\"hidden_layers\"]):\n",
    "            DW=np.dot(\n",
    "                np.multiply(\n",
    "                    self.network[\"output_layer\"][\"weights\"].T,\n",
    "                    DL\n",
    "                ),\n",
    "                self.Softmax(self.network[\"output_layer\"][\"z_state\"],True)\n",
    "            )\n",
    "\n",
    "            DB=np.sum(DW.T,axis=1,keepdims=True)\n",
    "\n",
    "            self.network[\"output_layer\"][\"weights\"]=(self.network[\"output_layer\"][\"weights\"].T-self.alpha*DW).T\n",
    "            self.network[\"output_layer\"][\"bias\"]=self.network[\"output_layer\"][\"bias\"]-self.alpha*DB\n",
    "\n",
    "            DA=np.multiply(\n",
    "                self.network[\"output_layer\"][\"weights\"].T.dot(DW.T),\n",
    "                self.network[\"hidden_layers\"][x][\"function\"](\n",
    "                    self.network[\"hidden_layers\"][x][\"z_state\"],True\n",
    "                )\n",
    "            )\n",
    "            DW=DA.T.dot(self.network[\"hidden_layers\"][x][\"activated_state\"])\n",
    "            DB=DW\n",
    "\n",
    "            self.network[\"hidden_layers\"][x][\"weights\"]=(self.network[\"hidden_layers\"][x][\"weights\"].T-self.alpha*DW).T\n",
    "            self.network[\"hidden_layers\"][x][\"bias\"]=(self.network[\"hidden_layers\"][x][\"bias\"].T-self.alpha*DB).reshape(len(DB),1)\n",
    "\n",
    "        elif x!=len(self.network[\"hidden_layers\"]) and x!=1:\n",
    "            DW=np.multiply(\n",
    "                self.network[\"hidden_layers\"][x+1][\"weights\"].T.dot(DW),\n",
    "                self.network[\"hidden_layers\"][x][\"function\"](\n",
    "                    self.network[\"hidden_layers\"][x][\"z_state\"],True\n",
    "                )\n",
    "            )\n",
    "            DW=DW*(self.network[\"hidden_layers\"][x][\"activated_state\"])\n",
    "            DB=DW\n",
    "            self.network[\"hidden_layers\"][x][\"weights\"]=(self.network[\"hidden_layers\"][x][\"weights\"].T-self.alpha*DW).T\n",
    "            self.network[\"hidden_layers\"][x][\"bias\"]=(self.network[\"hidden_layers\"][x][\"bias\"].T-self.alpha*DB).reshape(len(DB),1)\n",
    "\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a774c88-a011-452e-8eeb-e0b2bccd5ff8",
   "metadata": {},
   "source": [
    "There are two new things to point out here, one the weights are transposed and two nested multiplications within dot product of the output layer's weight gradient. The purpose of transposing is to make multiplication compatible between the weight gradient matrix and the weight matrix itself, since backpropagation operates in reverse order to that of forward propagation the weights are used in a mirrored manor. Where H1-H2 weights might be shape (20,30) during forward propagation, during backpropagation we are moving from H2-H1 so the weights appear as (30,20). During parameter updates, we would transpose the weights again subtracting the gradient multiplied by the learning rate, then re-transposing the weights to account for dimensions. Thus ensuring we can use the updated weights and bias the same exact way during the forward pass but under a new iteration/epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a5e1eb-dce8-4b07-ba02-6b8bd7e914f4",
   "metadata": {},
   "source": [
    "### Derivatives and the Softmax Jacobian matrix\n",
    "If you already have a step in the door to calculus then derivatives are pretty familiar, but what happens when we get a function that takes a vector as an input and produces a probability vector output? Well we get a matrix of partial derivatives or the Jacobian matrix....which introduces us to a new field of calculus, vector calculus, yayyyy. The only takeaway we need from vector calculus is how we obtain the gradient of a vector-valued function such as softmax accepting an arbitrary number of variables in a given vector and returning one of equal size. Because of this property, equal sizes, we can easily approximate the gradient through a matrix of partial derivatives of the softmax's output index with the index of the output. However since there are $NxN$ possible changes with respects to a given input/output pair we get a matrix of size $NxN$ which will look like the following:\n",
    "$$\n",
    "\\newcommand\\ColV[1]{\\begin{bmatrix}#1\\end{bmatrix}}\n",
    "J=[\\frac{∂f}{∂x_1}\\dots \\frac{∂f}{∂x_n}]=\\ColV{∇^Tf_1\\\\ \\vdots\\\\∇^Tf_n}=\\begin{bmatrix}\n",
    "\t\\frac{∂f_1}{∂x_1} & \\dots & \\frac{∂f_1}{∂x_n}\\\\\n",
    "\t\\dots & \\ddots & \\dots \\\\\n",
    "\t\\frac{∂f_n}{∂x_1} & \\dots & \\frac{∂f_n}{∂x_n}\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "For as scary as this looks its not that hard to implement and is under the Softmax function defined earlier as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456900eb-95ae-4120-9a11-dea33d302d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=len(vector)\n",
    "SM=self.Softmax(vector,derivative=False)\n",
    "JM=np.empty((n,n),dtype=type(SM))#initialize the NxN matrix\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i==j:\n",
    "            JM[i,j]=SM[i]*(1-SM[j])#the special case where ∂f_n and ∂x_n subscript n are the same index\n",
    "        else:\n",
    "            JM[i,j]=-SM[i]*SM[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e0790-0390-4e6a-9a0e-a4a4fab82166",
   "metadata": {},
   "source": [
    "This would impact the shape of the first gradient when performing backpropagation on the first output layer but since the dot product is performed the original weight shape is maintained even when this would have a arbitrary shape of $NxN$ because $N$ classes are what dictate the output layer weight shapes, the bias shapes, and the softmax input/output shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ba7e15-0170-4b63-b713-e5c1c1b4855b",
   "metadata": {},
   "source": [
    "Congratulations if you made it this far and your brain feels like this, the only thing left from here is exploring some problems with this backpropagation method, putting all the code together, and final remarks. (Or extra activation functions if you choose to read that section)\n",
    "\n",
    "\n",
    "![](https://media1.tenor.com/m/V40hZioO7S8AAAAd/spin-kermit-the-frog.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ce7c49-735a-4bc6-ad95-1bfc84cc2463",
   "metadata": {},
   "source": [
    "### The problems facing vanilla backpropagation and optimizers addressing these problems FINISH RMSPROP\n",
    "This design especially has multiple problems, notably [Exploding Gradients](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf), [Vanishing Gradients](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf), and [Saddle Points](https://arxiv.org/abs/2111.14069), [Local Minima](http://www.gatsby.ucl.ac.uk/~porbanz/teaching/UN3106S18/slides_08Feb.pdf)(even though most optimization algorithms can suffer from this problem). There are much different optimization techniques for various problems and even activation functions to address something like [vanishing gradients in ReLU solved with Swish](https://www.educative.io/answers/what-is-the-swish-activation-function), however activation function tuning can only go so far, we need a different type of optimization algorithm to efficiently and dynamically update the learning rate in order to converge faster to a local/global minima. A popular choice on the front end design of AI is the Adam optimizer, even [Tensorflow displayed Adam as an example optimizer on its website](https://www.tensorflow.org/) and for good reason. Adam or ADAptive Moment estimation, seems to merge STG and RMSProp (Stochastic Gradient Descent and Root Mean Squared Propagation) with a momentum factor added to the algorithm. I will be going further in depth about Adam in the coming notebooks but for now a good median would be RMSProp. RMSprop offers a pretty fast training rate and faster divergence toward a local/global minima, it takes the decaying average of the squares of previously calculated gradients then divides the learning rate by the resulting calculation, changing the learning rate dynamically. The formula for the updated parameter is as follows:\n",
    "$$E[g^2]_t=βE[g^2]_{t-1}+(1-β)\\frac{ẟC}{ẟW}^2$$\n",
    "\n",
    "Another pretty intimidating piece of math but again like vanilla backpropagation, its implementation is pretty straightforward as it only updates the learning rate with the following, note this is just pseudocode and is not a functional piece of code especially for this network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe43aae-411b-405f-abc0-72e3c7947384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#epsilon, rho, alpha, theta \n",
    "ε = 1e-12\n",
    "β = .9\n",
    "α = self.alpha\n",
    "#if L=O\n",
    "#RM=0\n",
    "#if L!=O\n",
    "RM=β*RM+(1-β)*(DW**2)\n",
    "#Parameter Theta\n",
    "RMS_Theta = Theta - α * DW / np.sqrt(RM+ε)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed7e43c-ea20-4f37-98e2-0744e5b1ef22",
   "metadata": {},
   "source": [
    "### Putting it a together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3904bc0d-b7a0-42a0-bf73-f1c5cf36ef34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 79.43918414645225 on epoch 0\n",
      "Loss: 79.43793883212028 on epoch 1\n",
      "Loss: 79.43669044499698 on epoch 2\n",
      "Loss: 79.43543896990833 on epoch 3\n",
      "Loss: 79.43418439156773 on epoch 4\n",
      "Loss: 79.43292669457485 on epoch 5\n",
      "Loss: 79.43166586341448 on epoch 6\n",
      "Loss: 79.43040188245575 on epoch 7\n",
      "Loss: 79.42913473595041 on epoch 8\n",
      "Loss: 79.42786440803226 on epoch 9\n",
      "Loss: 79.42659088271543 on epoch 10\n",
      "Loss: 79.42531414389346 on epoch 11\n",
      "Loss: 79.42403417533818 on epoch 12\n",
      "Loss: 79.42275096069804 on epoch 13\n",
      "Loss: 79.42146448349743 on epoch 14\n",
      "Loss: 79.42017472713508 on epoch 15\n",
      "Loss: 79.41888167488284 on epoch 16\n",
      "Loss: 79.41758530988415 on epoch 17\n",
      "Loss: 79.41628561515346 on epoch 18\n",
      "Loss: 79.41498257357395 on epoch 19\n",
      "Loss: 79.41367616789694 on epoch 20\n",
      "Loss: 79.41236638074014 on epoch 21\n",
      "Loss: 79.41105319458619 on epoch 22\n",
      "Loss: 79.40973659178178 on epoch 23\n",
      "Loss: 79.40841655453553 on epoch 24\n",
      "Loss: 79.40709306491732 on epoch 25\n",
      "Loss: 79.40576610485608 on epoch 26\n",
      "Loss: 79.4044356561387 on epoch 27\n",
      "Loss: 79.40310170040866 on epoch 28\n",
      "Loss: 79.40176421916439 on epoch 29\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "internal hyperparameter shapes for hidden layer 1:\n",
      "weights: (20, 784)\n",
      "bias: (20, 1)\n",
      "internal hyperparameter shapes for hidden layer 2:\n",
      "weights: (25, 20)\n",
      "bias: (25, 1)\n",
      "internal hyperparameter shapes for hidden layer 3:\n",
      "weights: (33, 25)\n",
      "bias: (33, 1)\n",
      "internal hyperparameter shapes for hidden layer 4:\n",
      "weights: (50, 33)\n",
      "bias: (50, 1)\n",
      "internal hyperparameter shapes for hidden layer 5:\n",
      "weights: (100, 50)\n",
      "bias: (100, 1)\n",
      "[3.17861516e-57 1.06944498e-60 1.34862970e-35 7.26199292e-28\n",
      " 9.99345880e-01 4.28847614e-51 4.77270230e-60 2.41117818e-41\n",
      " 7.82796613e-46 6.54119762e-04]\n",
      "Network prediction 4\n",
      "Real label: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#for fancy matplotlib effects\n",
    "plt.rcParams['grid.color']=\"green\"\n",
    "plt.rcParams['axes.facecolor']=\"black\"\n",
    "plt.rcParams['figure.facecolor']=\"black\"\n",
    "plt.rcParams['text.color']=\"green\"\n",
    "plt.rcParams['axes.labelcolor']=\"green\"\n",
    "plt.rcParams['xtick.color']=\"green\"\n",
    "plt.rcParams['ytick.color']=\"green\"\n",
    "\n",
    "\n",
    "class Network():\n",
    "    def __init__(self,\n",
    "        input_vector,\n",
    "        True_Label,\n",
    "        label_vector,\n",
    "        alpha,#learning rate for backpropagation\n",
    "        epochs,\n",
    "        seed\n",
    "    ):\n",
    "        #input processing\n",
    "        self.input_vector=input_vector\n",
    "        #one-hot encode the true value labels and the labels\n",
    "        encoded=np.zeros((len(label_vector),int(max(label_vector))+1))\n",
    "        encoded[np.arange(len(label_vector)),label_vector]=1\n",
    "        self.label_vector=encoded\n",
    "        encoded=np.zeros((len(True_Label),int(max(True_Label))+1))\n",
    "        encoded[np.arange(len(True_Label)),True_Label]=1\n",
    "        self.True_Label=encoded\n",
    "\n",
    "        #hyperparamaters\n",
    "        self.alpha=alpha\n",
    "        self.epochs=epochs\n",
    "\n",
    "        #set a seed for numpy generation to get reproducable results from random weight and bias generation\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.initialization={\n",
    "            \"Xavier\":lambda i,o: np.random.uniform(low=-np.sqrt(6/(i+o)),high=np.sqrt(6/(i+o)),size=(o,i)),\n",
    "            \"He\":lambda i,o: np.random.uniform(low=-np.sqrt(6/i),high=np.sqrt(6/i),size=(o,i))\n",
    "        }\n",
    "\n",
    "        self.Activations={\n",
    "            \"Tanh\": lambda x,derivative :\n",
    "                1-np.tanh(x)**2 if derivative \\\n",
    "                    else np.tanh(x)\n",
    "            ,\n",
    "\n",
    "            \"ReLU\": lambda zstate,derivative : np.array(\n",
    "                [0 if x<0 else 1 for x in zstate] if derivative \\\n",
    "                    else [0 if x<0 else x for x in zstate]\n",
    "            ),\n",
    "\n",
    "            \"Sigmoid\": lambda x, derivative : \n",
    "                1/1+np.exp(x)*(1-(1/1+np.exp(x))) if derivative \\\n",
    "                else 1/1+np.exp(-x)\n",
    "        }\n",
    "\n",
    "        self.preinitdict={\n",
    "            \"function\":None,#pointer to a lambda function chosen from the activation functions map\n",
    "            \"neurons\":0,#int\n",
    "            \"weights\":None,#2D matrix\n",
    "            \"bias\":None,#1D vector\n",
    "            \"z_state\":None,#1D array\n",
    "            \"activated_state\":None#1D array\n",
    "        }\n",
    "\n",
    "        self.network={\n",
    "            \"input_layer\":self.input_vector,\n",
    "            \"output_layer\":dict(self.preinitdict),\n",
    "            \"hidden_layers\":{},\n",
    "            \"LossFunction\":None\n",
    "        }\n",
    "        self.network[\"output_layer\"][\"neurons\"]=len(label_vector)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def generate_report(self):\n",
    "        r=[]\n",
    "        for x in OBJ.network[\"hidden_layers\"]:\n",
    "            r.append(f\"internal hyperparameter shapes for hidden layer {x}:\\nweights: {self.network['hidden_layers'][x]['weights'].shape}\\nbias: {self.network['hidden_layers'][x]['bias'].shape}\")\n",
    "        return r\n",
    "\n",
    "    def Softmax(self,vector,derivative=False):\n",
    "        if derivative:\n",
    "            n=len(vector)\n",
    "            SM=self.Softmax(vector,derivative=False)\n",
    "            JM=np.empty((n,n),dtype=type(SM))\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    if i==j:\n",
    "                        JM[i,j]=SM[i]*(1-SM[j])\n",
    "                    else:\n",
    "                        JM[i,j]=-SM[i]*SM[j]\n",
    "            return JM\n",
    "        else:\n",
    "            stable_exponential=np.exp(vector-np.max(vector))#prevent infinite overflow and underflow\n",
    "            return stable_exponential/np.sum(stable_exponential)\n",
    "        \n",
    "    def backpropagation(self,i):\n",
    "        DL=self.network[\"output_layer\"][\"activated_state\"]-self.True_Label[i]\n",
    "        for x in reversed(range(1,len(self.network[\"hidden_layers\"])+1)):\n",
    "            if x==len(self.network[\"hidden_layers\"]):\n",
    "                DW=np.dot(\n",
    "                    np.multiply(\n",
    "                        self.network[\"output_layer\"][\"weights\"].T,\n",
    "                        DL\n",
    "                    ),\n",
    "                    self.Softmax(self.network[\"output_layer\"][\"z_state\"],True)\n",
    "                )\n",
    "\n",
    "                DB=np.sum(DW.T,axis=1,keepdims=True)\n",
    "\n",
    "                self.network[\"output_layer\"][\"weights\"]=(self.network[\"output_layer\"][\"weights\"].T-self.alpha*DW).T\n",
    "                self.network[\"output_layer\"][\"bias\"]=self.network[\"output_layer\"][\"bias\"]-self.alpha*DB\n",
    "\n",
    "                DA=np.multiply(\n",
    "                    self.network[\"output_layer\"][\"weights\"].T.dot(DW.T),\n",
    "                    self.network[\"hidden_layers\"][x][\"function\"](\n",
    "                        self.network[\"hidden_layers\"][x][\"z_state\"],True\n",
    "                    )\n",
    "                )\n",
    "                DW=DA.T.dot(self.network[\"hidden_layers\"][x][\"activated_state\"])\n",
    "                DB=DW\n",
    "\n",
    "                self.network[\"hidden_layers\"][x][\"weights\"]=(self.network[\"hidden_layers\"][x][\"weights\"].T-self.alpha*DW).T\n",
    "                self.network[\"hidden_layers\"][x][\"bias\"]=(self.network[\"hidden_layers\"][x][\"bias\"].T-self.alpha*DB).reshape(len(DB),1)\n",
    "\n",
    "            elif x!=len(self.network[\"hidden_layers\"]) and x!=1:\n",
    "                DW=np.multiply(\n",
    "                    self.network[\"hidden_layers\"][x+1][\"weights\"].T.dot(DW),\n",
    "                    self.network[\"hidden_layers\"][x][\"function\"](\n",
    "                        self.network[\"hidden_layers\"][x][\"z_state\"],True\n",
    "                    )\n",
    "                )\n",
    "                DW=DW*(self.network[\"hidden_layers\"][x][\"activated_state\"])\n",
    "                DB=DW\n",
    "                self.network[\"hidden_layers\"][x][\"weights\"]=(self.network[\"hidden_layers\"][x][\"weights\"].T-self.alpha*DW).T\n",
    "                self.network[\"hidden_layers\"][x][\"bias\"]=(self.network[\"hidden_layers\"][x][\"bias\"].T-self.alpha*DB).reshape(len(DB),1)\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    def get_prediction(self,image):\n",
    "        for x in range(1,self.layercount):\n",
    "            z=np.empty(shape=self.network[\"hidden_layers\"][x][\"neurons\"])\n",
    "            for y in range(self.network[\"hidden_layers\"][x][\"neurons\"]):\n",
    "                if x==1:\n",
    "                    z.flat[y]=np.sum(np.dot(\n",
    "                        self.network[\"hidden_layers\"][x][\"weights\"][y],\n",
    "                        image\n",
    "                    )+self.network[\"hidden_layers\"][x][\"bias\"][y])\n",
    "                else:\n",
    "                    z.flat[y]=np.sum(np.dot(\n",
    "                        self.network[\"hidden_layers\"][x][\"weights\"][y],\n",
    "                        self.network[\"hidden_layers\"][x-1][\"activated_state\"]\n",
    "                    )+self.network[\"hidden_layers\"][x][\"bias\"][y])\n",
    "            self.network[\"hidden_layers\"][x][\"z_state\"]=z\n",
    "            self.network[\"hidden_layers\"][x][\"activated_state\"]=self.network[\"hidden_layers\"][x][\"function\"](z,False)\n",
    "\n",
    "        z=np.empty(shape=self.network[\"output_layer\"][\"neurons\"])\n",
    "        for y in range(self.network[\"output_layer\"][\"neurons\"]):\n",
    "            z.flat[y]=np.sum(np.multiply(\n",
    "                self.network[\"output_layer\"][\"weights\"][y],\n",
    "                self.network[\"hidden_layers\"][x][\"activated_state\"]\n",
    "            )+self.network[\"output_layer\"][\"bias\"][y])\n",
    "        self.network[\"output_layer\"][\"z_state\"]=z\n",
    "        self.network[\"output_layer\"][\"activated_state\"]=self.Softmax(z)\n",
    "        z=np.empty(shape=self.network[\"output_layer\"][\"neurons\"])\n",
    "        for y in range(self.network[\"output_layer\"][\"neurons\"]):\n",
    "            z.flat[y]=np.sum(np.multiply(\n",
    "                self.network[\"output_layer\"][\"weights\"][y],\n",
    "                self.network[\"hidden_layers\"][x][\"activated_state\"]\n",
    "            )+self.network[\"output_layer\"][\"bias\"][y])\n",
    "        self.network[\"output_layer\"][\"z_state\"]=z\n",
    "        self.network[\"output_layer\"][\"activated_state\"]=self.Softmax(z)\n",
    "        print(self.network[\"output_layer\"][\"activated_state\"])\n",
    "        predictedlabel=np.where(self.network[\"output_layer\"][\"activated_state\"]==max(self.network[\"output_layer\"][\"activated_state\"]))[0][0]\n",
    "        return predictedlabel\n",
    "\n",
    "    def initialize_layers(self,neurondist,layercount,activation):\n",
    "        self.layercount=layercount+1\n",
    "        if type(neurondist)==int:\n",
    "            neurondist=[neurondist//y for y in reversed([x for x in range(1,self.layercount)])]\n",
    "        if type(activation)==str:\n",
    "            if activation==\"Softmax\":\n",
    "                print(\"Cannot add softmax activation for hidden layer activation functions.\")\n",
    "                exit()\n",
    "            if type(activation)==list and len(activation)!=self.layercount:\n",
    "                print(\"The list of activation functions are not complete\")\n",
    "                exit()\n",
    "            else:\n",
    "                activation=[activation for y in range(1,self.layercount)]\n",
    "        for x in range(1,self.layercount):\n",
    "            self.network[\"hidden_layers\"][x]=dict(self.preinitdict)#shallow copy \n",
    "            self.network[\"hidden_layers\"][x][\"neurons\"]=neurondist[x-1]\n",
    "            args=[[neurondist[x-1],len(self.input_vector[0])] if x==1 else [neurondist[x-1],neurondist[x-2]]]\n",
    "            self.network[\"hidden_layers\"][x][\"function\"]=self.Activations[activation[x-1]]\n",
    "            if activation[x-1]==\"ReLU\" or activation[x-1]==\"Leaky ReLU\":\n",
    "                self.network[\"hidden_layers\"][x][\"weights\"]=self.initialization[\"He\"](args[0][1],args[0][0])\n",
    "                self.network[\"hidden_layers\"][x][\"bias\"]=self.initialization[\"He\"](1,args[0][0])\n",
    "            else:\n",
    "                self.network[\"hidden_layers\"][x][\"weights\"]=self.initialization[\"Xavier\"](args[0][1],args[0][0])\n",
    "                self.network[\"hidden_layers\"][x][\"bias\"]=self.initialization[\"Xavier\"](1,args[0][0])\n",
    "        self.network[\"output_layer\"][\"weights\"]=self.initialization[\"Xavier\"](args[0][0],len(self.label_vector))\n",
    "        self.network[\"output_layer\"][\"bias\"]=self.initialization[\"Xavier\"](1,len(self.label_vector))\n",
    "            \n",
    "\n",
    "    def Train(self):\n",
    "        for _ in range(0,self.epochs):\n",
    "            for i in range(len(self.input_vector)):\n",
    "                for x in range(1,self.layercount):\n",
    "                    z=np.empty(shape=self.network[\"hidden_layers\"][x][\"neurons\"])\n",
    "                    for y in range(self.network[\"hidden_layers\"][x][\"neurons\"]):\n",
    "                        if x==1:#weights shape (20,784), input vector shape = 784\n",
    "                            z.flat[y]=np.sum(np.dot(\n",
    "                                self.network[\"hidden_layers\"][x][\"weights\"][y],\n",
    "                                self.input_vector[i]\n",
    "                            )+self.network[\"hidden_layers\"][x][\"bias\"][y])\n",
    "                        else:\n",
    "                            z.flat[y]=np.sum(np.dot(\n",
    "                                self.network[\"hidden_layers\"][x][\"weights\"][y],\n",
    "                                self.network[\"hidden_layers\"][x-1][\"activated_state\"]\n",
    "                            )+self.network[\"hidden_layers\"][x][\"bias\"][y])\n",
    "                    self.network[\"hidden_layers\"][x][\"z_state\"]=z\n",
    "                    self.network[\"hidden_layers\"][x][\"activated_state\"]=self.network[\"hidden_layers\"][x][\"function\"](z,False)\n",
    "\n",
    "                z=np.empty(shape=self.network[\"output_layer\"][\"neurons\"])\n",
    "                for y in range(self.network[\"output_layer\"][\"neurons\"]):\n",
    "                    z.flat[y]=np.sum(np.multiply(\n",
    "                        self.network[\"output_layer\"][\"weights\"][y],\n",
    "                        self.network[\"hidden_layers\"][x][\"activated_state\"]\n",
    "                    )+self.network[\"output_layer\"][\"bias\"][y])\n",
    "                self.network[\"output_layer\"][\"z_state\"]=z\n",
    "                self.network[\"output_layer\"][\"activated_state\"]=self.Softmax(z)\n",
    "                self.backpropagation(i)\n",
    "\n",
    "            Loss=-np.sum(self.True_Label[i] * np.log(self.network[\"output_layer\"][\"activated_state\"]))\n",
    "            plt.plot(_,Loss,'go')\n",
    "            print(f'Loss: {Loss} on epoch {_}')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Gradient descent direction\")\n",
    "        plt.show()\n",
    "\n",
    "data=np.array(pd.read_csv(\"train.csv\"))\n",
    "np.random.shuffle(data)\n",
    "m,n=data.shape\n",
    "#m=number of examples\n",
    "#n=number of labels\n",
    "data.T \n",
    "#here we want to take 80% of the dataset in order to train it and 20% to validate\n",
    "#however this is computationally demanding depending on the structure of the network\n",
    "#so we will be taking 32 images for training or 0.076% of the data or a typical batch size\n",
    "Y_train=np.array([int(data[x][:1]) for x in range(round(len(data)*.00076))])\n",
    "X_train=np.array([data[x][1:]-np.mean(data[x][1:])/np.std(data[x][1:]) for x in range(round(len(data)*.00076))])\n",
    "#pull the next image from the randomized dataset\n",
    "#a batch size of 33 will be 0.078%\n",
    "#32 for training and 1 for validation\n",
    "Y_test=np.array([int(data[x][:1]) for x in range(round(len(data)*.00078))])[-1]\n",
    "X_test=np.array([data[x][1:]-np.mean(data[x][1:])/np.std(data[x][1:]) for x in range(round(len(data)*.00078))])[-1]\n",
    "OBJ = Network(\n",
    "    X_train,#normalized image data\n",
    "    Y_train,#true label vector\n",
    "    [0,1,2,3,4,5,6,7,8,9],#label vector for one-hot comparison\n",
    "    0.00037,#learning rate\n",
    "    30,#epochs\n",
    "    37#seed for numpy random\n",
    ")\n",
    "OBJ.initialize_layers(100,5,\"Tanh\")\n",
    "OBJ.Train()\n",
    "results=OBJ.generate_report()\n",
    "for x in results:\n",
    "    print(x)\n",
    "print(f\"Network prediction {OBJ.get_prediction(X_test)}\")\n",
    "print(f\"Real label: {Y_test}\")\n",
    "d=np.delete(data[32],0).reshape(28,28)\n",
    "plt.imshow(d,cmap='gray', vmin=0, vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a096d6a2-935c-491b-b4e6-e646c985e7dd",
   "metadata": {},
   "source": [
    "#### If you encouncer a FileNotFoundError\n",
    "Download the [train.csv file from the repo](https://github.com/Alt900/AIScratch/blob/main/Data/train.csv) and put the file in the same directory as the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c14fde3-fabd-44b5-b561-5c4ccfb1b920",
   "metadata": {},
   "source": [
    "### Some extra activation functions\n",
    "#### The piecewise class\n",
    "Activation functions in neural networks can be split into two classes, piecewise or single functions. Piecewise functions contain sub-functions depending on the domain the function would lie in given the input, for example the binary step, parametric relu, and elu functions:\n",
    "\n",
    "$ELU=\\begin{cases}x& x≥0 \\\\ α*(e^x-1)& x < 0\\end{cases}$\n",
    "\n",
    "$PReLU=\\begin{cases}x& x≥0 \\\\ α*x & x < 0\\end{cases}$\n",
    "\n",
    "$Binary step=\\begin{cases}1& x≥0 \\\\0 & x < 0\\end{cases}$\n",
    "\n",
    "These functions allow for a little bit more control over what data and features get passed onto the next layer in the network, these functions can be helpful in the first hidden layer as it would pick and choose more prominent features in the data allowing for greater abstraction of features. Generally single function activations like Tanh are pretty good at abstraction but piecewise \"kills\" irrelevant features that have too low of a score to pass onto the next layer adding a bit of extra complexity to the abstraction process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8da7a57-45e8-4ca6-a1ae-379a87d7cc7b",
   "metadata": {},
   "source": [
    "### Finally we are done\n",
    "And that be that, take this code and mess around with it, go nuts tuning hyperparameters, seeing different gradient problems, and attempt different solutions. Have fun with the notebook and peace.\n",
    "\n",
    "![](https://media1.tenor.com/m/YUzRkMOL-3EAAAAC/programming-computer-frog.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a830347-3bb9-4e83-94d7-b0c97d984e0f",
   "metadata": {},
   "source": [
    "# Extra resources\n",
    "\n",
    "[MIT Opencourseware, an amazing source for AI and mathematical topics](https://ocw.mit.edu/)\n",
    "\n",
    "[Weight initialization techniques](https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/)\n",
    "\n",
    "[Stanford CS229 Machine Learning course Lecture 12, backprop & improving neural networks](https://www.youtube.com/watch?v=zUazLXZZA2U)\n",
    "\n",
    "[Application of calculus in backpropagation](https://www.youtube.com/watch?v=157JtMJUlko)\n",
    "\n",
    "[3B1B's video series on neural networks](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4134cca9-fe5f-4d68-b376-c5d9151861a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
